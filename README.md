# ğŸ¤– BotTrainer â€“ LLM-Based NLU Model Trainer & Evaluator

BotTrainer is an **end-to-end LLM-powered Natural Language Understanding (NLU) platform** designed to perform **intent classification and entity extraction** using **prompt engineering** instead of traditional machine learning classifiers.

The system uses a **JSON-first dataset design**, a **local Gemma-3 model (via Ollama)**, and provides **real-time inference, evaluation dashboards, and dataset analytics** through an interactive **Streamlit UI**.

---

## ğŸš€ Key Highlights

- ğŸ”¹ LLM-based NLU (no classical ML intent classifiers)
- ğŸ”¹ JSON-driven intent & entity schema
- ğŸ”¹ Prompt-engineered structured output
- ğŸ”¹ Local inference using **Gemma-3**
- ğŸ”¹ Evaluation on multiple intents with metrics & confusion matrix
- ğŸ”¹ Clean, modular, production-style project structure
- ğŸ”¹ Interactive UI for testing, evaluation, and dataset insights

---

## ğŸ¯ Project Objectives

- Replace traditional intent classifiers with **LLM prompt-based inference**
- Perform **intent detection + entity extraction** in one pass
- Enforce **structured JSON output** from LLMs
- Evaluate NLU performance using:
  - Accuracy
  - Precision
  - Recall
  - F1-score
  - Confusion Matrix
- Provide a **real-time UI** for testing and evaluation
- Follow **industry-grade ML project structure**

---

## ğŸ§  System Architecture (High-Level)

```
User Input
   â†“
Prompt Template + Intent Schema
   â†“
Gemma-3 (Local LLM via Ollama)
   â†“
Structured JSON Output
   â†“
Intent & Entity Parsing
   â†“
Evaluation + UI Visualization
```

---

## ğŸ“¦ Dataset Design

### 1ï¸âƒ£ Primary Dataset â€“ `intents.json` (Core Dataset)

The system is **JSON-first**, meaning the LLM directly consumes the intent schema.

Each intent contains:
- Intent name
- Training examples
- Supported entity types

#### Example

```json
{
  "intents": [
    {
      "name": "book_flight",
      "examples": [
        "Book a flight to Delhi",
        "I want to fly to Mumbai tomorrow"
      ],
      "entities": ["location", "date"]
    }
  ],
  "entities": {
    "location": ["Delhi", "Mumbai", "Chennai"],
    "date": ["today", "tomorrow"]
  }
}
```

âœ” Used directly in prompt construction  
âœ” No vector DB or embeddings required  
âœ” Easily extensible to new domains  

---

### 2ï¸âƒ£ Flattened Dataset â€“ `full_nlu_dataset_200.csv`

Generated from `intents.json` for:

- Evaluation  
- Visualization  
- Dataset analysis  

**Schema:**

| Column | Description |
|------|------------|
| text | User utterance |
| true_intent | Ground truth intent |

---

## ğŸ§ª Evaluation Strategy

- **Total Intents:** 10  
- **Evaluation Samples:** 1 sample per intent  
- **Total Evaluation Size:** 10 samples  

This ensures:
- Every intent is validated at least once
- No bias from repeated samples
- Clear per-intent correctness verification

**Metrics Used:**
- Accuracy
- Precision (weighted)
- Recall (weighted)
- F1-score (weighted)
- Confusion Matrix

---

## ğŸ–¥ï¸ User Interface (Streamlit)

### ğŸ”¹ NLU Tester
- Real-time intent prediction
- Confidence score visualization
- Structured entity extraction display

<p align="center">
  <img src="assets/1.png" alt="NLU Tester â€“ Real-time Intent & Entity Prediction" width="850"/>
</p>

*Interactive interface for testing user queries and viewing predicted intent, confidence score, and extracted entities in real time.*

---

### ğŸ”¹ Model Evaluation Dashboard
- One-click evaluation
- Metric cards (Accuracy, Precision, Recall, F1)
- Confusion matrix table

<p align="center">
  <img src="assets/2.png" alt="Model Evaluation Dashboard with Metrics and Confusion Matrix" width="850"/>
</p>

*Provides a complete performance evaluation of the NLU model across all intents using standard classification metrics.*

---

### ğŸ”¹ Dataset Overview
- Intent distribution chart
- Sample dataset preview
- Dataset summary statistics

<p align="center">
  <img src="assets/3.png" alt="Dataset Overview â€“ Intent Distribution" width="850"/>
</p>

<p align="center">
  <img src="assets/4.png" alt="Dataset Sample Preview and Summary Statistics" width="850"/>
</p>

*Visual exploration of dataset balance, intent distribution, sample utterances, and overall dataset statistics.*


---

## ğŸ—‚ï¸ Project Structure

```
INFOSIS_BOTTRAINER/
â”‚
â”œâ”€â”€ assets/                    # UI screenshots
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml             # Model & path configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_data/
â”‚       â”œâ”€â”€ intents.json        # Core intent schema
â”‚       â””â”€â”€ full_nlu_dataset_200.csv
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ bottrainer_*.log        # Timestamped logs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ gemma_nlu.py        # LLM inference logic
â”‚   â”‚   â”œâ”€â”€ evaluator.py       # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ json_loader.py     # Dataset loader & validation
â”‚   â”‚   â””â”€â”€ json_to_dataframe.py
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ main_pipeline.py    # End-to-end evaluation pipeline
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ prompt_template.py  # Prompt engineering
â”‚       â”œâ”€â”€ logger.py           # Logging system
â”‚       â””â”€â”€ config_loader.py
â”œâ”€â”€ app.py                      # Streamlit application
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Setup & Execution

### 1ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Ensure Ollama & Gemma-3
```bash
ollama pull gemma3:latest
```

### 3ï¸âƒ£ Run the Application
```bash
streamlit run app.py
```

---

## ğŸ§© Technologies Used

- Python
- Streamlit
- Scikit-learn
- Pandas
- Ollama
- Gemma-3 LLM
- Prompt Engineering

---

## ğŸ“ Learning Outcomes

- Designing JSON-first NLU systems
- Prompt engineering for structured LLM outputs
- Evaluating LLM-based classifiers
- Building modular ML pipelines
- Developing production-ready Streamlit dashboards
- Debugging schema mismatches in real-world systems

---

## ğŸ‘¨â€ğŸ’» Author

**Jeeva M**  
AI / ML Engineer  
Portfolio Project  

---

## â­ Future Enhancements

- Entity-level evaluation metrics
- Confusion matrix heatmap
- Prompt inspection UI
- Model comparison (Gemma vs LLaMA vs GPT)
- Deployment via Docker or Hugging Face Spaces
